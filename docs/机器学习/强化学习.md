## 马尔科夫链
https://zhuanlan.zhihu.com/p/109217883
### S（state）状态
状态就是智能体观察到的当前环境的部分或者全部特征。

### A（action）动作
智能体做出的具体行为
`动作空间`就是该智能体能够做出的动作数量。
## R（reward）奖励
当我们在某个状态下，完成动作。环境就会给我们反馈，告诉我们这个动作的效果如何。这种效果的数值表达，就是奖励。
奖励的设定是主观的，也就是说我们为了智能体更好地学习工作，自己定的。

## Q&V
R只考虑当下
- 评估`动作`的价值，我们称为`Q值`：它代表了智能体选择这个动作后，一直到最终状态奖励总和的期望； 
- 评估`状态`的价值，我们称为`V值`：它代表了智能体在这个状态下，一直到最终状态的奖励总和的期望。
V值是会根据不同的策略有所变化的！
与V值不同，Q值和策略并没有直接相关，而与环境的状态转移概率相关，而环境的状态转移概率是不变的。

Q到V
$$
v_\pi(s)=\sum_{a\in A}\pi(a|s)q_\pi(s,a)
$$
V到Q
$$
q_\pi(s,a)=R_{s}^a+\gamma\sum_{s'}P_{ss'}^av_pi(s')
$$
## 蒙地卡罗(MC)估算状态V值
G值：从状态某个状态S到最终状态的总收获。

V是G的平均数。

## 时序差分(TD)估算状态V值

$$
V(S_t)=V(S_t)+\alpha[R_{t+1}+\gamma V(S_t+1)-V(S_t)]
$$
在TD(0)中，使用`下一状态的V`加上`状态转移奖励`作为更新目标
## Qlearning
需要一个QTable，存储Q值，其中每一行代表一个状态，每一列代表状态对应的行为的Q值，当遇到一个状态时，查询对应状态的所有应为的分数，按照最大的分数选择行为。
https://github.com/louisnino/RLcode/blob/master/tutorial_Qlearning.py
只适用于状态空间有限的情况
## DQN
Deep network + Qlearning = DQN

DQN更新: $Q(S,A)=Q(S,A)+\alpha[R+\gamma \max_{\alpha}Q(S',a)-Q(S,A)]$
- 其实DQN就是Qlearning扔掉Qtable，换上深度神经网络。
- 我们知道，解决连续型问题，如果表格不能表示，就用函数，而最好的函数就是深度神经网络。
- 和有监督学习不同，深度强化学习中，我们需要自己找更新目标。通常在马尔科夫链体系下，两个相邻状态状态差一个奖励r经常能被利用。

https://github.com/louisnino/RLcode/blob/master/tutorial_DQN.py

代码实现，以寻找二维平面最低点为例  
首先要定义环境，该环境包含智能体的状态，即位置。
环境要有一个reset函数，调用的时候会重新设置环境的状态。
环境还有有一个step函数，传入的是动作序号(动作的数量是一定的：上下左右移动)，该函数会对环境进行更新，并且返回更新后获得的新状态、奖励、以及是否结束。
```python
class Env:
    def __init__(self) -> None:
        ...
    
    def pes(self,pos):
        """势能面"""
        x,y=pos
        return x**2+y**2
    
    def reset(self):
        self.pos=np.random.rand(2)*10-5
        self.e=self.pes(self.pos)
        return self.pos
    
    def step(self,a:int):
        ms=np.array([
            [ 1,0],
            [-1,0],
            [0,1 ],
            [0,-1]
        ])*0.1
        pos=self.pos+ms[a]
        e=self.pes(pos)
        r=1 if (self.e-e)>0 else -1 #能量降低，奖励为正，能量升高，奖励为负
        d=True if (e> self.e+10) else False
        self.pos=pos
        self.e=e
        return pos,r,d
```
然后需要定义一个智能体，能够根据当前位置，进行分类，本例中为四分类，分别是每个每种行为对应的Q值
```python
class Net(torch.nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.linear=torch.nn.Sequential(
            torch.nn.Linear(2,2),
            torch.nn.ReLU(),
            torch.nn.Linear(2,4),
            torch.nn.ReLU(),
            torch.nn.Linear(4,4),
            torch.nn.Sigmoid()
        )
    
    def forward(self,pos:np.ndarray)->np.ndarray:
        pos=torch.from_numpy(pos).type(torch.float32)
        return self.linear(pos)
```
然后实例化环境和智能体，进行迭代训练，损失的计算方法如下：
1. 将当前的状态$S_t$代入智能体计算出向量$Q_t$
2. 选择其中最大的$q$值对应的行为指标$a_t$
3. 将$a_t$代入环境更新获得新的状态$S_{t+1}$以及奖励$r$
4. 将$S_{t+1}$代入智能体获得新的向量$Q_{t+1}$
5. 找出其q值最大的指标$a_{t+1}$
6. 拷贝一份原本的$Q$，记为$Q'$
7. 将$Q'[a]$替换为 $r+\lambda Q_{t+1}[a_{t+1}]$
8. $Q'$与$Q_t$之间的差距就是损失
```python
env=Env()
net=Net()
loss_fn=torch.nn.MSELoss()
optimer=torch.optim.Adam(net.parameters())

for i in range(1000):
    total_loss=0
    total_reward=0
    s=env.reset()
    for j in range(1000):
        Q=net(s) #Q值向量
        a=torch.argmax(Q).item() #行为指标
        q=Q[a] #行为对应的Q值
        s_,r,d=env.step(a) # 根据行为迭代一次环境
        total_reward+=r
        Q_=net(s_) #下一步的Q值向量
        a_=torch.argmax(Q_).item() # 下一步的行为指标
        q_=Q_[a_] #下一步行为的Q值
        targetQ=Q.detach().numpy().copy()
        targetQ[a]=r+0.8*q_
        loss=loss_fn(Q,torch.from_numpy(targetQ))
        optimer.zero_grad()
        loss.backward()
        optimer.step()
        total_loss+=loss.item()
        s=s_
    print(f'{total_loss:>10.4f}    {total_reward:<4}    {i}/1000')
torch.save(net.state_dict(),'DGN.pt')
```
## Actor-Critic
Actor-Critic，其实是用了两个网络：

两个网络有一个共同点，输入状态S:
- Actor，输出行为向量
- Critic，输出分数值

TD-error: $\gamma \times V(s') + r - V(s)$  
Critic的任务就是让TD-error尽量小。然后TD-error给Actor做更新。
## DDPG